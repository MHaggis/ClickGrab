name: Nightly ClickGrab Analysis

on:
  schedule:
    # Runs "At 01:00 UTC every day"
    - cron: '0 1 * * *'
  workflow_dispatch:

permissions:
  contents: write 

jobs:
  # Job 1: Fetch and scan URLs from feeds
  fetch_and_scan:
    name: Fetch & Scan URLs
    runs-on: ubuntu-latest
    timeout-minutes: 45
    outputs:
      scan_date: ${{ steps.set_date.outputs.date }}
      has_results: ${{ steps.check_results.outputs.has_results }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for cache file

      - name: Set date output
        id: set_date
        run: echo "date=$(date +%Y-%m-%d)" >> $GITHUB_OUTPUT

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create required directories
        run: |
          mkdir -p nightly_reports
          mkdir -p analysis
          echo "Created required directories"

      - name: Run ClickGrab Scan (URLs only, new domains only)
        id: run_scan
        timeout-minutes: 40
        run: |
          TODAY=$(date +%Y-%m-%d)
          echo "üîç Scanning URLs for $TODAY"
          
          # Set encoding for proper UTF-8 handling
          export PYTHONIOENCODING=utf-8
          
          # Run scan - the caching mechanism in collect_clickfix_gist_urls() ensures
          # we only scan NEW domains that weren't in the last run's cache
          python clickgrab.py \
            --download \
            --clickfix-gist \
            --output-dir nightly_reports \
            --format json \
            --limit 100 \
            --debug
          
          echo "‚úÖ ClickGrab scan complete"

      - name: Check if we have results
        id: check_results
        run: |
          if [ -n "$(find nightly_reports -name '*.json' -type f 2>/dev/null)" ]; then
            echo "has_results=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Found scan results"
          else
            echo "has_results=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è  No new domains to scan (cache hit)"
          fi

      - name: Standardize report filenames
        if: steps.check_results.outputs.has_results == 'true'
        run: |
          TODAY=$(date +%Y-%m-%d)
          
          # Find the latest JSON report
          LATEST_JSON=$(find nightly_reports -name "*.json" -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -d' ' -f2-)
          
          if [ -n "$LATEST_JSON" ]; then
            echo "üìÑ Found report: $LATEST_JSON"
            
            # Create standardized filename
            cp "$LATEST_JSON" "nightly_reports/clickgrab_report_${TODAY}.json"
            cp "$LATEST_JSON" "latest_consolidated_report.json"
            
            # Show summary
            echo "=== Report Summary ==="
            jq -r '.summary // {} | to_entries | .[] | "\(.key): \(.value)"' "$LATEST_JSON" 2>/dev/null || echo "No summary"
            echo "Total sites: $(jq -r '.sites | length' "$LATEST_JSON" 2>/dev/null || echo '0')"
            echo "===================="
          fi

      - name: Upload scan results
        if: steps.check_results.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: scan-results-${{ steps.set_date.outputs.date }}
          path: |
            nightly_reports/*.json
            latest_consolidated_report.json
            analysis/clickfix_gist_cache.json
          retention-days: 7

  # Job 2: Advanced analysis and blog generation
  analyze_and_blog:
    name: Analyze & Generate Blog
    needs: fetch_and_scan
    if: needs.fetch_and_scan.outputs.has_results == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create directories
        run: |
          mkdir -p nightly_reports
          mkdir -p analysis

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: scan-results-${{ needs.fetch_and_scan.outputs.scan_date }}
          path: .

      - name: Run advanced analysis
        timeout-minutes: 15
        run: |
          TODAY="${{ needs.fetch_and_scan.outputs.scan_date }}"
          echo "üìä Running advanced threat analysis for $TODAY"
          
          # Generate blog content from scan results
          python bin/analyze.py -d "$TODAY" -v
          
          echo "‚úÖ Advanced analysis complete"

      - name: Upload analysis results
        uses: actions/upload-artifact@v4
        with:
          name: analysis-results-${{ needs.fetch_and_scan.outputs.scan_date }}
          path: |
            analysis/*.json
            analysis/*.md
          retention-days: 7

  # Job 3: Build static site and publish
  build_and_publish:
    name: Build Site & Publish
    needs: [fetch_and_scan, analyze_and_blog]
    if: needs.fetch_and_scan.outputs.has_results == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create directories
        run: |
          mkdir -p nightly_reports
          mkdir -p analysis
          mkdir -p docs/assets/images
          mkdir -p public

      - name: Copy logo files
        run: |
          if [ -f "assets/logo.png" ]; then
            cp assets/logo.png docs/assets/images/logo.png
            echo "‚úÖ Copied logo"
          elif [ -f "assets/images/logo.png" ]; then
            cp assets/images/logo.png docs/assets/images/logo.png
            echo "‚úÖ Copied logo"
          else
            echo "‚ö†Ô∏è  Logo not found"
          fi

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: scan-results-${{ needs.fetch_and_scan.outputs.scan_date }}
          path: .

      - name: Download analysis results
        uses: actions/download-artifact@v4
        with:
          name: analysis-results-${{ needs.fetch_and_scan.outputs.scan_date }}
          path: analysis/

      - name: Build static website
        timeout-minutes: 35
        run: |
          echo "üèóÔ∏è  Building static website"
          echo "Note: build.py processes the 30 most recent reports for speed"
          
          # Generate complete website with Jinja2 templates
          time python bin/build.py
          
          echo "‚úÖ Website build complete"

      - name: Sync public to docs (GitHub Pages)
        run: |
          if [ -d "public" ]; then
            echo "üì¶ Syncing public/ to docs/"
            
            # Clean docs directory (preserve .git)
            find docs -mindepth 1 -not -path 'docs/.git*' -delete 2>/dev/null || true
            
            # Copy all files
            cp -r public/* docs/
            
            # Verify
            echo "Public reports: $(ls public/reports 2>/dev/null | wc -l || echo 0)"
            echo "Docs reports: $(ls docs/reports 2>/dev/null | wc -l || echo 0)"
            echo "‚úÖ Sync complete"
          else
            echo "‚ùå No public/ directory found"
            exit 1
          fi

      - name: Verify generated files
        run: |
          echo "üîç Verifying generated files"
          
          # Check key files
          [ -f "docs/index.html" ] && echo "‚úÖ index.html" || echo "‚ùå Missing index.html"
          [ -f "docs/analysis.html" ] && echo "‚úÖ analysis.html" || echo "‚ùå Missing analysis.html"
          [ -f "docs/techniques.html" ] && echo "‚úÖ techniques.html" || echo "‚ùå Missing techniques.html"
          [ -f "latest_consolidated_report.json" ] && echo "‚úÖ latest_consolidated_report.json" || echo "‚ùå Missing report"
          
          echo ""
          echo "üìÅ Sample generated files:"
          find docs -type f -name "*.html" | head -5

      - name: Commit and push results
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          TODAY="${{ needs.fetch_and_scan.outputs.scan_date }}"
          
          # Add all generated files
          git add latest_consolidated_report.json
          git add nightly_reports/
          git add docs/
          git add analysis/
          git add public/ || true
          
          # Commit if changes exist
          if ! git diff --staged --quiet; then
            echo "üìù Committing changes"
            git commit -m "chore: nightly analysis results ($TODAY)" \
                       -m "- Scanned new ClickFix domains from gist" \
                       -m "- Advanced threat intelligence analysis" \
                       -m "- Updated static website" \
                       -m "- Auto-generated blog posts"
            git push
            echo "‚úÖ Successfully pushed changes"
          else
            echo "‚ÑπÔ∏è  No changes to commit"
          fi

  # Summary job (always runs to report status)
  summary:
    name: Workflow Summary
    needs: [fetch_and_scan, analyze_and_blog, build_and_publish]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Report status
        run: |
          echo "=== Nightly Analysis Summary ==="
          echo "Date: ${{ needs.fetch_and_scan.outputs.scan_date }}"
          echo "Scan job: ${{ needs.fetch_and_scan.result }}"
          echo "Analysis job: ${{ needs.analyze_and_blog.result }}"
          echo "Build job: ${{ needs.build_and_publish.result }}"
          echo ""
          
          if [ "${{ needs.fetch_and_scan.outputs.has_results }}" == "true" ]; then
            echo "‚úÖ New domains were scanned and processed"
          else
            echo "‚ÑπÔ∏è  No new domains found (cache hit - all domains already analyzed)"
          fi
          echo "==============================="
